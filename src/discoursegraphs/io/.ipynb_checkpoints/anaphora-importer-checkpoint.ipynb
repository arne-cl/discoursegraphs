{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import re\n",
      "import difflib\n",
      "from operator import itemgetter\n",
      "from collections import defaultdict\n",
      "from networkx import MultiDiGraph\n",
      "\n",
      "ANNOTATION_REGEX = re.compile('/\\w')\n",
      "\n",
      "ANAPHORA_ANNOTATION_DIR = os.path.expanduser('~/repos/abstract-anaphora-annotation/')\n",
      "DAS_FILE = os.path.join(ANAPHORA_ANNOTATION_DIR, '174pcc.orig.utf8.txt.melanie_das_annotation')\n",
      "ES_FILE = os.path.join(ANAPHORA_ANNOTATION_DIR, '174pcc.orig.utf8.txt.melanie_es_annotation')\n",
      "UNANNOTATED_FILE = os.path.join(ANAPHORA_ANNOTATION_DIR, '174pcc.orig.utf8.txt')\n",
      "\n",
      "TOKENIZED_DATA_DIR = os.path.expanduser('~/repos/pcc-annis-merged/maz176/tokenized')\n",
      "TOKENIZED_FILES = !ls $TOKENIZED_DATA_DIR/*.txt\n",
      "\n",
      "OUTPUT_ROOTDIR = os.path.expanduser('~/repos/pcc-annis-merged/maz176/anaphora/tosik/')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "das_lines = open(DAS_FILE, 'r').readlines()\n",
      "unannotated_lines = open(UNANNOTATED_FILE, 'r').readlines()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generate a dictionary of primary texts from the unannotated file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "docs = defaultdict(list) # starts counting at 1\n",
      "doc_count = 0\n",
      "for line in unannotated_lines:\n",
      "    if line.isupper(): # every document starts with the author name (ALL CAPS)\n",
      "        doc_count += 1\n",
      "    docs[doc_count].append(line)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for i, doc_lines in docs.items():\n",
      "#     with open(os.path.join(ANAPHORA_ANNOTATION_DIR, \"primary_text{0:03d}.txt\".format(i)), 'w') as outfile:\n",
      "#         outfile.writelines(doc_lines)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generate a dictionary of headlines from those extracted primary texts:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "headlines = defaultdict(str) # starts counting at 1\n",
      "for doc_id, doc_lines in docs.items():\n",
      "    headlines[doc_id] = doc_lines[2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generate a dictionary of annotated texts from the annotation file\n",
      "and match it against the primary texts. (The author names are missing\n",
      "from the annotated texts)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ANNOTATION_REGEX = re.compile('/\\w')\n",
      "\n",
      "annotated_docs = defaultdict(list)\n",
      "annotated_doc_count = 0\n",
      "for annotated_line in das_lines:\n",
      "    original_line = headlines[annotated_doc_count+1].strip() # headlines start counting at 1\n",
      "    # annotation not only adds '/xyz' to a word, but also messes up spacing\n",
      "    cleaned_annotated_line = ' '.join(ANNOTATION_REGEX.sub('', annotated_line).strip().split())\n",
      "\n",
      "    # fuzzy matching works fine\n",
      "#     if difflib.SequenceMatcher(None, original_line, cleaned_annotated_line).ratio() >= 0.9:\n",
      "#         print annotated_doc_count, cleaned_annotated_line, ' || ',\n",
      "#         annotated_doc_count += 1\n",
      "\n",
      "    if original_line == cleaned_annotated_line:\n",
      "        annotated_doc_count += 1\n",
      "    annotated_docs[annotated_doc_count].append(annotated_line)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_unannotated_docs(unannotated_file):\n",
      "    docs = defaultdict(list) # starts counting at 1\n",
      "    doc_count = 0\n",
      "    with open(unannotated_file, 'r') as unannotated:\n",
      "        unannotated_lines = unannotated.readlines()\n",
      "        for line in unannotated_lines:\n",
      "            if line.isupper(): # every document starts with the author name (ALL CAPS)\n",
      "                doc_count += 1\n",
      "            docs[doc_count].append(line)\n",
      "    return docs\n",
      "\n",
      "def get_headlines(unannotated_docs):\n",
      "    headlines = defaultdict(str) # starts counting at 1\n",
      "    for doc_id, doc_lines in unannotated_docs.items():\n",
      "        headlines[doc_id] = doc_lines[2]\n",
      "    return headlines\n",
      "    \n",
      "def get_annotated_docs(headlines, annotated_file):\n",
      "    with open(annotated_file, 'r') as annotated:\n",
      "        annotated_lines = annotated.readlines()\n",
      "        annotated_docs = defaultdict(list)\n",
      "        annotated_doc_count = 0\n",
      "        for annotated_line in annotated_lines:\n",
      "            original_line = headlines[annotated_doc_count+1].strip() # headlines start counting at 1\n",
      "            # annotation not only adds '/xyz' to a word, but also messes up spacing\n",
      "            cleaned_annotated_line = ' '.join(ANNOTATION_REGEX.sub('', annotated_line).strip().split())\n",
      "        \n",
      "            if original_line == cleaned_annotated_line:\n",
      "                annotated_doc_count += 1\n",
      "            annotated_docs[annotated_doc_count].append(annotated_line)\n",
      "        return annotated_docs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unannotated_docs = get_unannotated_docs(UNANNOTATED_FILE)\n",
      "headlines = get_headlines(unannotated_docs)\n",
      "annotated_das_docs = get_annotated_docs(headlines, DAS_FILE)\n",
      "annotated_es_docs = get_annotated_docs(headlines, ES_FILE)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(docs) == len(annotated_das_docs) == len(annotated_es_docs)\n",
      "print len(docs), len(annotated_das_docs), len(annotated_das_docs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n",
        "170 170 170\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Match annotated documents to tokenized text files (and their MAZ IDs!)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fixed: Two files (00001, 00002) did have their headlines on the first line (instead of the third).\n",
      "\n",
      "## we need more than title matching\n",
      "\n",
      "There are two documents with the same headline:\n",
      "\n",
      "```\n",
      "maz-18160.tok.txt\n",
      "3:Alarmsignal\n",
      "5:Es brodelt an der CDU-Basis in Werder und Umgebung .  Der Grund ist die Weigerung des Innenministers und Parteivorsitzenden J\u00f6rg Sch\u00f6nbohm , dem eindeutigen B\u00fcrgerwillen der Golmer nach Eingliederung in die Stadt Werder nachzukommen .  Ein Alarmsignal f\u00fcr Sch\u00f6nbohm m\u00fcsste es jedenfalls sein , wenn sich jetzt die Junge Union in Werder scharf gegen seinen Kurs wendet und ein CDU-Stadtverordneter gar mit seinem sofortigen Austritt aus der Christlich Demokratischen Union droht , weil f\u00fcr ihn demokratische Spielregeln eklatant verletzt wurden .\n",
      "\n",
      "maz-18914.tok.txt\n",
      "3:Alarmsignal\n",
      "5:Mit Helga Kaden streicht eine der namhaftesten Gesch\u00e4ftsleute der Stadt die Segel .  Sie konnte ihr traditionsreiches Gesch\u00e4ft wegen der anhaltenden Kundenflaute nicht mehr \u00fcber Wasser halten .  Mag sein , das ihr Konzept nicht aufging .  Die exklusiven Sachen hatten ihren Preis .  Nur - es gab ja Zeiten , da verkaufte sich das sehr gut . Es wird schon gemunkelt , dass weitere dem Beispiel folgen werden .  Ausgerechnet in der sch\u00f6nen Weihnachtszeit offenbart sich das Elend des innerst\u00e4dtischen Einzelhandels .  Und zwar an einer Ecke , die eine Art Aush\u00e4ngeschild f\u00fcr die Stadt ist .  Ein leerer Laden ist alles andere als einladend f\u00fcr den Schwenk in die City .  Ein Alarmsignal f\u00fcr den neuen B\u00fcrgermeister .  Die Stadt kann nur bedingt helfen , aber sie muss es endlich tun .  Erstens : Die Parkgeb\u00fchren in der Einkaufszone m\u00fcssen weg .  Zweitens : Ein anziehender Branchenmix muss her - wo es die Stadt nicht selbst in der Hand hat , muss sie es mehr fordern , notfalls den Eigent\u00fcmern geeignete gewerbliche Mieter selbst vermitteln .  Drittens : Im Umgang mit hilfesuchenden H\u00e4ndlern muss der Grundsatz gelten - nichts ist unm\u00f6glich .  So l\u00e4sst sich das schlingernde City-Schiff vielleicht doch noch auf einen erfolgversprechenden Kurs bringen .\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_tokenized_docs(tokenized_files):\n",
      "    tokenized_docs = defaultdict(lambda : defaultdict()) # defaultdict of defaultdicts\n",
      "    for fpath in tokenized_files:\n",
      "        doc_id = re.search('\\d+', os.path.basename(fpath)).group()\n",
      "        tokenized_docs[doc_id]['path'] = fpath\n",
      "        with open(fpath, 'r') as tokenized_file:\n",
      "            tokenized_docs[doc_id]['lines'] = tokenized_file.readlines()\n",
      "            tokenized_docs[doc_id]['headline'] = tokenized_docs[doc_id]['lines'][2]\n",
      "    return tokenized_docs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unannotated_headlines = get_headlines(unannotated_docs)\n",
      "tokenized_docs = get_tokenized_docs(TOKENIZED_FILES)\n",
      "\n",
      "def map_unannotated_to_tokenized(unannotated_headlines, tokenized_docs):\n",
      "    headline_to_unanno_doc_id = defaultdict(list)\n",
      "    for unannotated_doc_id, unannotated_headline in unannotated_headlines.items():\n",
      "        headline_to_unanno_doc_id[unannotated_headline].append(unannotated_doc_id)\n",
      "    \n",
      "    unannotated_to_tokenized = defaultdict(list)\n",
      "    for i, tokenized_doc_id in enumerate(sorted(tokenized_docs.keys())):\n",
      "        tokenized_headline = tokenized_docs[tokenized_doc_id]['headline']\n",
      "        if tokenized_headline in headline_to_unanno_doc_id:\n",
      "            unannotated_doc_ids = headline_to_unanno_doc_id[tokenized_headline]\n",
      "            for unannotated_doc_id in unannotated_doc_ids:\n",
      "                unannotated_to_tokenized[unannotated_doc_id].append(tokenized_doc_id)\n",
      "        else:\n",
      "            for unanno_headline, unannotated_doc_ids in headline_to_unanno_doc_id.items():\n",
      "                if difflib.SequenceMatcher(None, tokenized_headline, unanno_headline).ratio() >= 0.9:\n",
      "                    for unannotated_doc_id in unannotated_doc_ids:\n",
      "                        unannotated_to_tokenized[unannotated_doc_id].append(tokenized_doc_id)\n",
      "                    break\n",
      "    return unannotated_to_tokenized"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unannotated_to_tokenized = map_unannotated_to_tokenized(unannotated_headlines, tokenized_docs)\n",
      "\n",
      "unannotated_docs = get_unannotated_docs(UNANNOTATED_FILE)\n",
      "for unannotated_doc_id in unannotated_docs:\n",
      "    if unannotated_doc_id not in unannotated_to_tokenized:\n",
      "        print \"unannotated_doc_id: \", unannotated_doc_id, unannotated_docs[unannotated_doc_id][2]\n",
      "        print \"\\tdoesn't (closely) match any tokenized headline\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(unannotated_docs)\n",
      "print len(unannotated_to_tokenized.keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "170\n",
        "170\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The headline 'Alarmsignale' is used in two texts:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(headline_to_unanno_doc_id)\n",
      "for headline in headline_to_unanno_doc_id:\n",
      "    if len(headline_to_unanno_doc_id[headline]) != 1:\n",
      "        print headline.strip(), headline_to_unanno_doc_id[headline]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'headline_to_unanno_doc_id' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-14-80f3273e80d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheadline_to_unanno_doc_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mheadline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mheadline_to_unanno_doc_id\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheadline_to_unanno_doc_id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mheadline\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[0mheadline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheadline_to_unanno_doc_id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mheadline\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'headline_to_unanno_doc_id' is not defined"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def disambiguate_matches(unannotated_to_tokenized, unannotated_docs, tokenized_docs):\n",
      "    \"\"\"\n",
      "    returns a dictionary that unambigiously maps from document IDs\n",
      "    used by Christian Dittrichs anaphora annotation tool to the document IDs\n",
      "    officially used in MAZ176.\n",
      "    \"\"\"\n",
      "    for unannotated_doc_id, tokenized_doc_ids in unannotated_to_tokenized.items():\n",
      "        # there's more than one tokenized text with the same headline\n",
      "        if len(tokenized_doc_ids) > 1:\n",
      "            unannotated_text = unannotated_docs[unannotated_doc_id][4]\n",
      "            candidate_scores = []\n",
      "            for tokenized_doc_id in tokenized_doc_ids:\n",
      "                tokenized_text = tokenized_docs[tokenized_doc_id]['lines'][4]\n",
      "                candidate_score = difflib.SequenceMatcher(None, unannotated_text, tokenized_text).ratio()\n",
      "                candidate_scores.append((tokenized_doc_id, candidate_score))\n",
      "            # choose the most likely candidate (based on string similarity of the text)\n",
      "            # max over values, cf. http://stackoverflow.com/a/13145419\n",
      "            unannotated_to_tokenized[unannotated_doc_id] = max(candidate_scores,key=itemgetter(1))[0]\n",
      "        else: # there's only one doc with this headline\n",
      "            unannotated_to_tokenized[unannotated_doc_id] = tokenized_doc_ids[0]\n",
      "    \n",
      "    return unannotated_to_tokenized"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Matching seems to work fine now, regardless of the tokenization:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokenized_docs = get_tokenized_docs(TOKENIZED_FILES)\n",
      "unannotated_to_tokenized = map_unannotated_to_tokenized(unannotated_headlines, tokenized_docs)\n",
      "\n",
      "unannotated_docs = get_unannotated_docs(UNANNOTATED_FILE)\n",
      "\n",
      "unanno2tok = disambiguate_matches(unannotated_to_tokenized, unannotated_docs, tokenized_docs)\n",
      "\n",
      "for unanno_doc_id, tok_doc_id in unanno2tok.items():\n",
      "    unanno_headline = unannotated_docs[unanno_doc_id][2]\n",
      "    tok_headline = tokenized_docs[tok_doc_id]['lines'][2]\n",
      "    if unanno_headline != tok_headline:\n",
      "        print unanno_headline, tok_headline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unannotated_docs = get_unannotated_docs(UNANNOTATED_FILE)\n",
      "unannotated_headlines = get_headlines(unannotated_docs)\n",
      "tokenized_docs = get_tokenized_docs(TOKENIZED_FILES)\n",
      "\n",
      "unannotated_to_tokenized = map_unannotated_to_tokenized(unannotated_headlines, tokenized_docs)\n",
      "unanno2tok = disambiguate_matches(unannotated_to_tokenized, unannotated_docs, tokenized_docs)\n",
      "\n",
      "annotated_das_docs = get_annotated_docs(unannotated_headlines, DAS_FILE)\n",
      "\n",
      "# write DAS annotation files\n",
      "# for das_doc_id in annotated_das_docs:\n",
      "#     tok_doc_id = unanno2tok[das_doc_id]\n",
      "#     output_filepath = os.path.join(OUTPUT_ROOTDIR, 'das',\n",
      "#                                    \"maz-{}.anaphora.das.txt\".format(tok_doc_id))\n",
      "#     with open(output_filepath, 'w') as outfile:\n",
      "#         outfile.writelines(annotated_das_docs[das_doc_id])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# write ESS annotation files\n",
      "# annotated_es_docs = get_annotated_docs(headlines, ES_FILE)\n",
      "# for es_doc_id in annotated_es_docs:\n",
      "#     tok_doc_id = unanno2tok[es_doc_id]\n",
      "#     output_filepath = os.path.join(OUTPUT_ROOTDIR, 'es',\n",
      "#                                    \"maz-{}.anaphora.es.txt\".format(tok_doc_id))\n",
      "#     with open(output_filepath, 'w') as outfile:\n",
      "#         outfile.writelines(annotated_es_docs[es_doc_id])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Repair spacing in annotation files\n",
      "\n",
      "no space after annotation:\n",
      "```\n",
      "maz-14654.anaphora.das.txt\n",
      "Elisabeth Herzog-von der Heide (SPD) freute sich \u00fcber das/nWahlergebnis, das/rihr mehr als 60 Prozent einbrachte\n",
      "```\n",
      "\n",
      "too much space after annotation:\n",
      "```\n",
      "maz-7690.anaphora.das.txt\n",
      "Das/n  Schlachtefest in Paaren/Glien st\u00fctzt diese These.\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "annotations = {'n': 'nominal',\n",
      "               'a': 'abstract',\n",
      "               'r': 'relative',\n",
      "               'p': 'pleonastic'}\n",
      "\n",
      "no_space = \"das/nWahlergebnis, das/r?ihr mehr als das/a?fetzt\"\n",
      "too_much = \"Das/n  Schlachtefest in Paaren/Glien, das/a?  fetzt!\"\n",
      "\n",
      "NO_SPACE_AFTER_DAS_REGEX = re.compile('([Dd]as/[anr]\\??)(\\w)')\n",
      "NO_SPACE_AFTER_ES_REGEX = re.compile('([Ee]s/[anp]\\??)(\\w)')\n",
      "NO_SPACE_AFTER_ANNOTATION_REPL = r'\\1 \\2'\n",
      "\n",
      "print NO_SPACE_AFTER_DAS_REGEX.sub(NO_SPACE_AFTER_ANNOTATION_REPL, no_space)\n",
      "print NO_SPACE_AFTER_DAS_REGEX.sub(NO_SPACE_AFTER_ANNOTATION_REPL, too_much)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "das/n Wahlergebnis, das/r? ihr mehr als das/a? fetzt\n",
        "Das/n  Schlachtefest in Paaren/Glien, das/a?  fetzt!\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Repair DAS annotated files:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DAS_DIR = os.path.join(OUTPUT_ROOTDIR, 'das')\n",
      "DAS_FILES = !ls $DAS_DIR/*.das.txt\n",
      "\n",
      "for das_filepath in DAS_FILES:\n",
      "    with open(das_filepath, 'r') as das_file:\n",
      "        fixed_lines = []\n",
      "        for line in das_file.readlines():\n",
      "            tokenized_line = ' '.join(line.split())+'\\n' # remove superfluous spacing between tokens\n",
      "            annotated_line = NO_SPACE_AFTER_DAS_REGEX.sub(NO_SPACE_AFTER_ANNOTATION_REPL, tokenized_line)\n",
      "            fixed_lines.append(annotated_line)\n",
      "    with open(das_filepath, 'w') as das_file:\n",
      "        das_file.writelines(fixed_lines)\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Repair ES annotated files:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ES_DIR = os.path.join(OUTPUT_ROOTDIR, 'es')\n",
      "ES_FILES = !ls $ES_DIR/*.es.txt\n",
      "\n",
      "for es_filepath in ES_FILES:\n",
      "    with open(es_filepath, 'r') as es_file:\n",
      "        fixed_lines = []\n",
      "        for line in es_file.readlines():\n",
      "            tokenized_line = ' '.join(line.split())+'\\n' # remove superfluous spacing between tokens\n",
      "            annotated_line = NO_SPACE_AFTER_ES_REGEX.sub(NO_SPACE_AFTER_ANNOTATION_REPL, tokenized_line)\n",
      "            fixed_lines.append(annotated_line)\n",
      "    with open(es_filepath, 'w') as es_file:\n",
      "        es_file.writelines(fixed_lines)\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Convert Annotation files into graphs\n",
      "\n",
      "## tokenize annotated files to match our base tokenization\n",
      "\n",
      "For whatever reason, Christian Dittrich's tool uses untokenized files as input!\n",
      "Stefanie Dipper's tokenizer seems to work well with these files,\n",
      "but we need to add an empty line between the headline and the rest of the document!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DAS_DIR = os.path.join(OUTPUT_ROOTDIR, 'das')\n",
      "DAS_FILES = !ls $DAS_DIR/*.das.txt\n",
      "\n",
      "ES_DIR = os.path.join(OUTPUT_ROOTDIR, 'es')\n",
      "ES_FILES = !ls $ES_DIR/*.es.txt\n",
      "\n",
      "# add an empty line between headline and text in each document\n",
      "for filelist in (DAS_FILES, ES_FILES):\n",
      "    for filepath in filelist:\n",
      "        with open(filepath, 'r') as infile:\n",
      "            lines = infile.readlines()\n",
      "        with open(filepath, 'w') as outfile:\n",
      "            outfile.write(lines[0]+'\\n')\n",
      "            outfile.writelines(lines[1:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class AnaphoraDocumentGraph(MultiDiGraph):\n",
      "    \"\"\"\n",
      "    represents a text annotated with abstract anaphora types\n",
      "    as a graph\n",
      "    \"\"\"\n",
      "    def __init__(self, anaphora_filepath):\n",
      "        # super calls __init__() of base class MultiDiGraph\n",
      "        super(AnaphoraDocumentGraph, self).__init__()\n",
      "        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}