{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import re\n",
      "import difflib\n",
      "from operator import itemgetter\n",
      "from collections import defaultdict\n",
      "from networkx import MultiDiGraph\n",
      "\n",
      "ANNOTATION_REGEX = re.compile('/\\w')\n",
      "\n",
      "ANAPHORA_ANNOTATION_DIR = os.path.expanduser('~/repos/abstract-anaphora-annotation/')\n",
      "DAS_FILE = os.path.join(ANAPHORA_ANNOTATION_DIR, '174pcc.orig.utf8.txt.melanie_das_annotation')\n",
      "ES_FILE = os.path.join(ANAPHORA_ANNOTATION_DIR, '174pcc.orig.utf8.txt.melanie_es_annotation')\n",
      "UNANNOTATED_FILE = os.path.join(ANAPHORA_ANNOTATION_DIR, '174pcc.orig.utf8.txt')\n",
      "\n",
      "TOKENIZED_DATA_DIR = os.path.expanduser('~/repos/pcc-annis-merged/maz176/tokenized')\n",
      "TOKENIZED_FILES = !ls $TOKENIZED_DATA_DIR/*.txt\n",
      "\n",
      "OUTPUT_ROOTDIR = os.path.expanduser('~/repos/pcc-annis-merged/maz176/anaphora/tosik/')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "das_lines = open(DAS_FILE, 'r').readlines()\n",
      "unannotated_lines = open(UNANNOTATED_FILE, 'r').readlines()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generate a dictionary of primary texts from the unannotated file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "docs = defaultdict(list) # starts counting at 1\n",
      "doc_count = 0\n",
      "for line in unannotated_lines:\n",
      "    if line.isupper(): # every document starts with the author name (ALL CAPS)\n",
      "        doc_count += 1\n",
      "    docs[doc_count].append(line)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for i, doc_lines in docs.items():\n",
      "#     with open(os.path.join(ANAPHORA_ANNOTATION_DIR, \"primary_text{0:03d}.txt\".format(i)), 'w') as outfile:\n",
      "#         outfile.writelines(doc_lines)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generate a dictionary of headlines from those extracted primary texts:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "headlines = defaultdict(str) # starts counting at 1\n",
      "for doc_id, doc_lines in docs.items():\n",
      "    headlines[doc_id] = doc_lines[2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generate a dictionary of annotated texts from the annotation file\n",
      "and match it against the primary texts. (The author names are missing\n",
      "from the annotated texts)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ANNOTATION_REGEX = re.compile('/\\w')\n",
      "\n",
      "annotated_docs = defaultdict(list)\n",
      "annotated_doc_count = 0\n",
      "for annotated_line in das_lines:\n",
      "    original_line = headlines[annotated_doc_count+1].strip() # headlines start counting at 1\n",
      "    # annotation not only adds '/xyz' to a word, but also messes up spacing\n",
      "    cleaned_annotated_line = ' '.join(ANNOTATION_REGEX.sub('', annotated_line).strip().split())\n",
      "\n",
      "    # fuzzy matching works fine\n",
      "#     if difflib.SequenceMatcher(None, original_line, cleaned_annotated_line).ratio() >= 0.9:\n",
      "#         print annotated_doc_count, cleaned_annotated_line, ' || ',\n",
      "#         annotated_doc_count += 1\n",
      "\n",
      "    if original_line == cleaned_annotated_line:\n",
      "        annotated_doc_count += 1\n",
      "    annotated_docs[annotated_doc_count].append(annotated_line)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_unannotated_docs(unannotated_file):\n",
      "    docs = defaultdict(list) # starts counting at 1\n",
      "    doc_count = 0\n",
      "    with open(unannotated_file, 'r') as unannotated:\n",
      "        unannotated_lines = unannotated.readlines()\n",
      "        for line in unannotated_lines:\n",
      "            if line.isupper(): # every document starts with the author name (ALL CAPS)\n",
      "                doc_count += 1\n",
      "            docs[doc_count].append(line)\n",
      "    return docs\n",
      "\n",
      "def get_headlines(unannotated_docs):\n",
      "    headlines = defaultdict(str) # starts counting at 1\n",
      "    for doc_id, doc_lines in unannotated_docs.items():\n",
      "        headlines[doc_id] = doc_lines[2]\n",
      "    return headlines\n",
      "    \n",
      "def get_annotated_docs(headlines, annotated_file):\n",
      "    with open(annotated_file, 'r') as annotated:\n",
      "        annotated_lines = annotated.readlines()\n",
      "        annotated_docs = defaultdict(list)\n",
      "        annotated_doc_count = 0\n",
      "        for annotated_line in annotated_lines:\n",
      "            original_line = headlines[annotated_doc_count+1].strip() # headlines start counting at 1\n",
      "            # annotation not only adds '/xyz' to a word, but also messes up spacing\n",
      "            cleaned_annotated_line = ' '.join(ANNOTATION_REGEX.sub('', annotated_line).strip().split())\n",
      "        \n",
      "            if original_line == cleaned_annotated_line:\n",
      "                annotated_doc_count += 1\n",
      "            annotated_docs[annotated_doc_count].append(annotated_line)\n",
      "        return annotated_docs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unannotated_docs = get_unannotated_docs(UNANNOTATED_FILE)\n",
      "headlines = get_headlines(unannotated_docs)\n",
      "annotated_das_docs = get_annotated_docs(headlines, DAS_FILE)\n",
      "annotated_es_docs = get_annotated_docs(headlines, ES_FILE)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(docs) == len(annotated_das_docs) == len(annotated_es_docs)\n",
      "print len(docs), len(annotated_das_docs), len(annotated_das_docs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n",
        "170 170 170\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Match annotated documents to tokenized text files (and their MAZ IDs!)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fixed: Two files (00001, 00002) did have their headlines on the first line (instead of the third).\n",
      "\n",
      "## we need more than title matching\n",
      "\n",
      "There are two documents with the same headline:\n",
      "\n",
      "```\n",
      "maz-18160.tok.txt\n",
      "3:Alarmsignal\n",
      "5:Es brodelt an der CDU-Basis in Werder und Umgebung ...\n",
      "\n",
      "maz-18914.tok.txt\n",
      "3:Alarmsignal\n",
      "5:Mit Helga Kaden streicht eine der namhaftesten Gesch\u00e4ftsleute der Stadt die Segel ...\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_tokenized_docs(tokenized_files):\n",
      "    tokenized_docs = defaultdict(lambda : defaultdict()) # defaultdict of defaultdicts\n",
      "    for fpath in tokenized_files:\n",
      "        doc_id = re.search('\\d+', os.path.basename(fpath)).group()\n",
      "        tokenized_docs[doc_id]['path'] = fpath\n",
      "        with open(fpath, 'r') as tokenized_file:\n",
      "            tokenized_docs[doc_id]['lines'] = tokenized_file.readlines()\n",
      "            tokenized_docs[doc_id]['headline'] = tokenized_docs[doc_id]['lines'][2]\n",
      "    return tokenized_docs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unannotated_headlines = get_headlines(unannotated_docs)\n",
      "tokenized_docs = get_tokenized_docs(TOKENIZED_FILES)\n",
      "\n",
      "def map_unannotated_to_tokenized(unannotated_headlines, tokenized_docs):\n",
      "    headline_to_unanno_doc_id = defaultdict(list)\n",
      "    for unannotated_doc_id, unannotated_headline in unannotated_headlines.items():\n",
      "        headline_to_unanno_doc_id[unannotated_headline].append(unannotated_doc_id)\n",
      "    \n",
      "    unannotated_to_tokenized = defaultdict(list)\n",
      "    for i, tokenized_doc_id in enumerate(sorted(tokenized_docs.keys())):\n",
      "        tokenized_headline = tokenized_docs[tokenized_doc_id]['headline']\n",
      "        if tokenized_headline in headline_to_unanno_doc_id:\n",
      "            unannotated_doc_ids = headline_to_unanno_doc_id[tokenized_headline]\n",
      "            for unannotated_doc_id in unannotated_doc_ids:\n",
      "                unannotated_to_tokenized[unannotated_doc_id].append(tokenized_doc_id)\n",
      "        else:\n",
      "            for unanno_headline, unannotated_doc_ids in headline_to_unanno_doc_id.items():\n",
      "                if difflib.SequenceMatcher(None, tokenized_headline, unanno_headline).ratio() >= 0.9:\n",
      "                    for unannotated_doc_id in unannotated_doc_ids:\n",
      "                        unannotated_to_tokenized[unannotated_doc_id].append(tokenized_doc_id)\n",
      "                    break\n",
      "    return unannotated_to_tokenized, headline_to_unanno_doc_id"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unannotated_to_tokenized, headline_to_unanno_doc_id = map_unannotated_to_tokenized(unannotated_headlines, tokenized_docs)\n",
      "\n",
      "unannotated_docs = get_unannotated_docs(UNANNOTATED_FILE)\n",
      "for unannotated_doc_id in unannotated_docs:\n",
      "    if unannotated_doc_id not in unannotated_to_tokenized:\n",
      "        print \"unannotated_doc_id: \", unannotated_doc_id, unannotated_docs[unannotated_doc_id][2]\n",
      "        print \"\\tdoesn't (closely) match any tokenized headline\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(unannotated_docs)\n",
      "print len(unannotated_to_tokenized.keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "170\n",
        "170\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The headline 'Alarmsignale' is used in two texts:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(headline_to_unanno_doc_id)\n",
      "for headline in headline_to_unanno_doc_id:\n",
      "    if len(headline_to_unanno_doc_id[headline]) != 1:\n",
      "        print headline.strip(), headline_to_unanno_doc_id[headline]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "169\n",
        "Alarmsignal [84, 95]\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def disambiguate_matches(unannotated_to_tokenized, unannotated_docs, tokenized_docs):\n",
      "    \"\"\"\n",
      "    returns a dictionary that unambigiously maps from document IDs\n",
      "    used by Christian Dittrichs anaphora annotation tool to the document IDs\n",
      "    officially used in MAZ176.\n",
      "    \"\"\"\n",
      "    for unannotated_doc_id, tokenized_doc_ids in unannotated_to_tokenized.items():\n",
      "        # there's more than one tokenized text with the same headline\n",
      "        if len(tokenized_doc_ids) > 1:\n",
      "            unannotated_text = unannotated_docs[unannotated_doc_id][4]\n",
      "            candidate_scores = []\n",
      "            for tokenized_doc_id in tokenized_doc_ids:\n",
      "                tokenized_text = tokenized_docs[tokenized_doc_id]['lines'][4]\n",
      "                candidate_score = difflib.SequenceMatcher(None, unannotated_text, tokenized_text).ratio()\n",
      "                candidate_scores.append((tokenized_doc_id, candidate_score))\n",
      "            # choose the most likely candidate (based on string similarity of the text)\n",
      "            # max over values, cf. http://stackoverflow.com/a/13145419\n",
      "            unannotated_to_tokenized[unannotated_doc_id] = max(candidate_scores,key=itemgetter(1))[0]\n",
      "        else: # there's only one doc with this headline\n",
      "            unannotated_to_tokenized[unannotated_doc_id] = tokenized_doc_ids[0]\n",
      "    \n",
      "    return unannotated_to_tokenized"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Matching seems to work fine now, regardless of the tokenization:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokenized_docs = get_tokenized_docs(TOKENIZED_FILES)\n",
      "unannotated_to_tokenized, _ = map_unannotated_to_tokenized(unannotated_headlines, tokenized_docs)\n",
      "\n",
      "unannotated_docs = get_unannotated_docs(UNANNOTATED_FILE)\n",
      "\n",
      "unanno2tok = disambiguate_matches(unannotated_to_tokenized, unannotated_docs, tokenized_docs)\n",
      "\n",
      "for unanno_doc_id, tok_doc_id in unanno2tok.items():\n",
      "    unanno_headline = unannotated_docs[unanno_doc_id][2]\n",
      "    tok_headline = tokenized_docs[tok_doc_id]['lines'][2]\n",
      "    if unanno_headline != tok_headline:\n",
      "        print unanno_headline, tok_headline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "F\u00fcrchtet euch nicht!\n",
        "F\u00fcrchtet euch nicht ! \n",
        "\n",
        "Steuer frei!\n",
        "Steuer frei ! \n",
        "\n",
        "\"Schmierentheater\"\n",
        "\" Schmierentheater \"\n",
        "\n",
        "Aber doch nicht bei uns!\n",
        "Aber doch nicht bei uns ! \n",
        "\n",
        "B\u00f6se, die nicht b\u00f6se sind\n",
        "B\u00f6se , die nicht b\u00f6se sind\n",
        "\n",
        "Eigentlich fehlt ja keiner...\n",
        "Eigentlich fehlt ja keiner ... \n",
        "\n",
        "Sp\u00e4hen, nicht spitzeln\n",
        "Sp\u00e4hen , nicht spitzeln\n",
        "\n",
        "Des einen Freud . . .\n",
        "Des einen Freud ... \n",
        "\n",
        "Nicht vergessen!\n",
        "Nicht vergessen ! \n",
        "\n",
        "Gebt endlich Gummi!\n",
        "Gebt endlich Gummi ! \n",
        "\n",
        "Vielleicht geht's gut\n",
        "Vielleicht geht 's gut\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unannotated_docs = get_unannotated_docs(UNANNOTATED_FILE)\n",
      "unannotated_headlines = get_headlines(unannotated_docs)\n",
      "tokenized_docs = get_tokenized_docs(TOKENIZED_FILES)\n",
      "\n",
      "unannotated_to_tokenized, _ = map_unannotated_to_tokenized(unannotated_headlines, tokenized_docs)\n",
      "unanno2tok = disambiguate_matches(unannotated_to_tokenized, unannotated_docs, tokenized_docs)\n",
      "\n",
      "annotated_das_docs = get_annotated_docs(unannotated_headlines, DAS_FILE)\n",
      "\n",
      "# write DAS annotation files\n",
      "# for das_doc_id in annotated_das_docs:\n",
      "#     tok_doc_id = unanno2tok[das_doc_id]\n",
      "#     output_filepath = os.path.join(OUTPUT_ROOTDIR, 'das',\n",
      "#                                    \"maz-{}.anaphora.das.txt\".format(tok_doc_id))\n",
      "#     with open(output_filepath, 'w') as outfile:\n",
      "#         outfile.writelines(annotated_das_docs[das_doc_id])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# write ESS annotation files\n",
      "# annotated_es_docs = get_annotated_docs(headlines, ES_FILE)\n",
      "# for es_doc_id in annotated_es_docs:\n",
      "#     tok_doc_id = unanno2tok[es_doc_id]\n",
      "#     output_filepath = os.path.join(OUTPUT_ROOTDIR, 'es',\n",
      "#                                    \"maz-{}.anaphora.es.txt\".format(tok_doc_id))\n",
      "#     with open(output_filepath, 'w') as outfile:\n",
      "#         outfile.writelines(annotated_es_docs[es_doc_id])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Repair spacing in annotation files\n",
      "\n",
      "no space after annotation:\n",
      "```\n",
      "maz-14654.anaphora.das.txt\n",
      "Elisabeth Herzog-von der Heide (SPD) freute sich \u00fcber das/nWahlergebnis, das/rihr mehr als 60 Prozent einbrachte\n",
      "```\n",
      "\n",
      "too much space after annotation:\n",
      "```\n",
      "maz-7690.anaphora.das.txt\n",
      "Das/n  Schlachtefest in Paaren/Glien st\u00fctzt diese These.\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "annotations = {'n': 'nominal',\n",
      "               'a': 'abstract',\n",
      "               'r': 'relative',\n",
      "               'p': 'pleonastic'}\n",
      "\n",
      "no_space = \"das/nWahlergebnis, das/r?ihr mehr als das/a?fetzt\"\n",
      "too_much = \"Das/n  Schlachtefest in Paaren/Glien, das/a?  fetzt!\"\n",
      "\n",
      "NO_SPACE_AFTER_DAS_REGEX = re.compile('([Dd]as/[anr]\\??)(\\w)')\n",
      "NO_SPACE_AFTER_ES_REGEX = re.compile('([Ee]s/[anp]\\??)(\\w)')\n",
      "NO_SPACE_AFTER_ANNOTATION_REPL = r'\\1 \\2'\n",
      "\n",
      "print NO_SPACE_AFTER_DAS_REGEX.sub(NO_SPACE_AFTER_ANNOTATION_REPL, no_space)\n",
      "print NO_SPACE_AFTER_DAS_REGEX.sub(NO_SPACE_AFTER_ANNOTATION_REPL, too_much)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Repair DAS annotated files:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DAS_DIR = os.path.join(OUTPUT_ROOTDIR, 'das')\n",
      "DAS_FILES = !ls $DAS_DIR/*.das.txt\n",
      "\n",
      "# for das_filepath in DAS_FILES:\n",
      "#     with open(das_filepath, 'r') as das_file:\n",
      "#         fixed_lines = []\n",
      "#         for line in das_file.readlines():\n",
      "#             tokenized_line = ' '.join(line.split())+'\\n' # remove superfluous spacing between tokens\n",
      "#             annotated_line = NO_SPACE_AFTER_DAS_REGEX.sub(NO_SPACE_AFTER_ANNOTATION_REPL, tokenized_line)\n",
      "#             fixed_lines.append(annotated_line)\n",
      "#     with open(das_filepath, 'w') as das_file:\n",
      "#         das_file.writelines(fixed_lines)\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Repair ES annotated files:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ES_DIR = os.path.join(OUTPUT_ROOTDIR, 'es')\n",
      "ES_FILES = !ls $ES_DIR/*.es.txt\n",
      "\n",
      "# for es_filepath in ES_FILES:\n",
      "#     with open(es_filepath, 'r') as es_file:\n",
      "#         fixed_lines = []\n",
      "#         for line in es_file.readlines():\n",
      "#             tokenized_line = ' '.join(line.split())+'\\n' # remove superfluous spacing between tokens\n",
      "#             annotated_line = NO_SPACE_AFTER_ES_REGEX.sub(NO_SPACE_AFTER_ANNOTATION_REPL, tokenized_line)\n",
      "#             fixed_lines.append(annotated_line)\n",
      "#     with open(es_filepath, 'w') as es_file:\n",
      "#         es_file.writelines(fixed_lines)\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Convert Annotation files into graphs\n",
      "\n",
      "## tokenize annotated files to match our base tokenization\n",
      "\n",
      "For whatever reason, Christian Dittrich's tool uses untokenized files as input!\n",
      "Stefanie Dipper's tokenizer seems to work well with these files,\n",
      "but we need to add an empty line between the headline and the rest of the document!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DAS_DIR = os.path.join(OUTPUT_ROOTDIR, 'das')\n",
      "DAS_FILES = !ls $DAS_DIR/*.das.txt\n",
      "\n",
      "ES_DIR = os.path.join(OUTPUT_ROOTDIR, 'es')\n",
      "ES_FILES = !ls $ES_DIR/*.es.txt\n",
      "\n",
      "# add an empty line between headline and text to each document\n",
      "# for filelist in (DAS_FILES, ES_FILES):\n",
      "#     for filepath in filelist:\n",
      "#         with open(filepath, 'r') as infile:\n",
      "#             lines = infile.readlines()\n",
      "#         with open(filepath, 'w') as outfile:\n",
      "#             outfile.write(lines[0]+'\\n')\n",
      "#             outfile.writelines(lines[1:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we're able to tokenize the annotated files with Stefanie Dipper's tokenizer:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "~/repos/pcc-annis-merged/maz176/anaphora/tosik/das $ ls *.txt | parallel ~/repos/dipper-tokenizer/tokenize.perl {} {.}.tok\n",
      "~/repos/pcc-annis-merged/maz176/anaphora/tosik/das $ ls *.tok | parallel mv {} {.}.txt\n",
      "\n",
      "~/repos/pcc-annis-merged/maz176/anaphora/tosik/es $ ls *.txt | parallel ~/repos/dipper-tokenizer/tokenize.perl {} {.}.tok\n",
      "~/repos/pcc-annis-merged/maz176/anaphora/tosik/es $ ls *.tok | parallel mv {} {.}.txt"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try to match the annotated files against the tokenized ones:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unannotated_docs = get_unannotated_docs(UNANNOTATED_FILE)\n",
      "unannotated_headlines = get_headlines(unannotated_docs)\n",
      "tokenized_docs = get_tokenized_docs(TOKENIZED_FILES)\n",
      "\n",
      "unannotated_to_tokenized, _ = map_unannotated_to_tokenized(unannotated_headlines, tokenized_docs)\n",
      "unanno2tok = disambiguate_matches(unannotated_to_tokenized, unannotated_docs, tokenized_docs)\n",
      "\n",
      "annotated_das_docs = get_annotated_docs(unannotated_headlines, DAS_FILE)\n",
      "\n",
      "DAS_DIR = os.path.join(OUTPUT_ROOTDIR, 'das')\n",
      "das_files = !ls $DAS_DIR/*.das.txt\n",
      "\n",
      "for annotated_doc_id in annotated_das_docs:\n",
      "    maz_id = unanno2tok[annotated_doc_id]\n",
      "#     print maz_id\n",
      "#     maz_id = re.search('\\d+', os.path.basename(das_filepath)).group()\n",
      "    annotated_lines = annotated_das_docs[annotated_doc_id]\n",
      "    if str(maz_id) not in tokenized_docs:\n",
      "        print \"{0} not in tokenized docs\".format(maz_id)\n",
      "\n",
      "# print tokenized_docs.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokenized_docs['14853']['lines']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 65,
       "text": [
        "['\\n',\n",
        " '\\n',\n",
        " 'Ein sch\\xc3\\xb6ner Start\\n',\n",
        " '\\n',\n",
        " 'Es h\\xc3\\xa4tte f\\xc3\\xbcr Luckenwaldes designierte B\\xc3\\xbcrgermeisterin Elisabeth Herzog-von_der_Heide nicht besser kommen k\\xc3\\xb6nnen .  Gleich am ersten Tag nach ihrem Wahlsieg kam - so als ob es so geplant gewesen w\\xc3\\xa4re - der EU-Kommissar f\\xc3\\xbcr Regionalpolitik aus Br\\xc3\\xbcssel , um hier mit Vertretern der letzten drei deutschen Urban-St\\xc3\\xa4dte die Programm-Vertr\\xc3\\xa4ge zu unterzeichnen .  Nat\\xc3\\xbcrlich war es so nicht geplant gewesen , sondern ein rein zuf\\xc3\\xa4lliges Zusammentreffen zweier f\\xc3\\xbcr Luckenwalde bedeutsamer Ereignisse , an denen Elisabeth Herzog nicht unbeteiligt war .\\n',\n",
        " '\\n',\n",
        " 'Dass Luckenwalde als kleinste von insgesamt zw\\xc3\\xb6lf deutschen St\\xc3\\xa4dten vom Urban-II-Projekt profitiert , ist ganz wesentlich der 2. Beigeordneten und k\\xc3\\xbcnftigen B\\xc3\\xbcrgermeisterin zu verdanken , was ihr im Wahlkampf sicherlich auch geholfen hat .  Brandenburgs St\\xc3\\xa4dtebauminister Hartmut Meyer meinte dann auch gestern :  \" Es ist ein guter Start , wenn man gleich am ersten Tag 38 Millionen Mark bekommt .  Wenn das so weiter geht ... \"\\n',\n",
        " '\\n',\n",
        " 'Nat\\xc3\\xbcrlich geht es nicht so weiter , jedenfalls nicht Tag f\\xc3\\xbcr Tag .  Und nach all den Vorschusslorbeeren meinte Minister Meyer dann auch zur k\\xc3\\xbcnftigen B\\xc3\\xbcrgermeisterin :  \" Jetzt werden Sie erst einmal an Ihren Taten gemessen . \"  \\n',\n",
        " '\\n']"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class AnaphoraDocumentGraph(MultiDiGraph):\n",
      "    \"\"\"\n",
      "    represents a text annotated with abstract anaphora types\n",
      "    as a graph\n",
      "    \"\"\"\n",
      "    def __init__(self, anaphora_filepath):\n",
      "        # super calls __init__() of base class MultiDiGraph\n",
      "        super(AnaphoraDocumentGraph, self).__init__()\n",
      "        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    }
   ],
   "metadata": {}
  }
 ]
}