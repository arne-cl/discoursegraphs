{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import re\n",
      "import difflib\n",
      "import itertools\n",
      "from operator import itemgetter\n",
      "from collections import defaultdict\n",
      "import networkx\n",
      "from networkx import MultiDiGraph\n",
      "\n",
      "ANNOTATION_REGEX = re.compile('/\\w')\n",
      "\n",
      "ANAPHORA_ANNOTATION_DIR = os.path.expanduser('~/repos/abstract-anaphora-annotation/')\n",
      "DAS_FILE = os.path.join(ANAPHORA_ANNOTATION_DIR, '174pcc.orig.utf8.txt.melanie_das_annotation')\n",
      "ES_FILE = os.path.join(ANAPHORA_ANNOTATION_DIR, '174pcc.orig.utf8.txt.melanie_es_annotation')\n",
      "UNANNOTATED_FILE = os.path.join(ANAPHORA_ANNOTATION_DIR, '174pcc.orig.utf8.txt')\n",
      "\n",
      "TOKENIZED_DATA_DIR = os.path.expanduser('~/repos/pcc-annis-merged/maz176/tokenized')\n",
      "TOKENIZED_FILES = !ls $TOKENIZED_DATA_DIR/*.txt\n",
      "\n",
      "OUTPUT_ROOTDIR = os.path.expanduser('~/repos/pcc-annis-merged/maz176/anaphora/tosik/')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 190
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "das_lines = open(DAS_FILE, 'r').readlines()\n",
      "unannotated_lines = open(UNANNOTATED_FILE, 'r').readlines()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generate a dictionary of primary texts from the unannotated file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "docs = defaultdict(list) # starts counting at 1\n",
      "doc_count = 0\n",
      "for line in unannotated_lines:\n",
      "    if line.isupper(): # every document starts with the author name (ALL CAPS)\n",
      "        doc_count += 1\n",
      "    docs[doc_count].append(line)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for i, doc_lines in docs.items():\n",
      "#     with open(os.path.join(ANAPHORA_ANNOTATION_DIR, \"primary_text{0:03d}.txt\".format(i)), 'w') as outfile:\n",
      "#         outfile.writelines(doc_lines)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generate a dictionary of headlines from those extracted primary texts:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "headlines = defaultdict(str) # starts counting at 1\n",
      "for doc_id, doc_lines in docs.items():\n",
      "    headlines[doc_id] = doc_lines[2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generate a dictionary of annotated texts from the annotation file\n",
      "and match it against the primary texts. (The author names are missing\n",
      "from the annotated texts)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ANNOTATION_REGEX = re.compile('/\\w')\n",
      "\n",
      "annotated_docs = defaultdict(list)\n",
      "annotated_doc_count = 0\n",
      "for annotated_line in das_lines:\n",
      "    original_line = headlines[annotated_doc_count+1].strip() # headlines start counting at 1\n",
      "    # annotation not only adds '/xyz' to a word, but also messes up spacing\n",
      "    cleaned_annotated_line = ' '.join(ANNOTATION_REGEX.sub('', annotated_line).strip().split())\n",
      "\n",
      "    # fuzzy matching works fine\n",
      "#     if difflib.SequenceMatcher(None, original_line, cleaned_annotated_line).ratio() >= 0.9:\n",
      "#         print annotated_doc_count, cleaned_annotated_line, ' || ',\n",
      "#         annotated_doc_count += 1\n",
      "\n",
      "    if original_line == cleaned_annotated_line:\n",
      "        annotated_doc_count += 1\n",
      "    annotated_docs[annotated_doc_count].append(annotated_line)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_unannotated_docs(unannotated_file):\n",
      "    docs = defaultdict(list) # starts counting at 1\n",
      "    doc_count = 0\n",
      "    with open(unannotated_file, 'r') as unannotated:\n",
      "        unannotated_lines = unannotated.readlines()\n",
      "        for line in unannotated_lines:\n",
      "            if line.isupper(): # every document starts with the author name (ALL CAPS)\n",
      "                doc_count += 1\n",
      "            docs[doc_count].append(line)\n",
      "    return docs\n",
      "\n",
      "def get_headlines(unannotated_docs):\n",
      "    headlines = defaultdict(str) # starts counting at 1\n",
      "    for doc_id, doc_lines in unannotated_docs.items():\n",
      "        headlines[doc_id] = doc_lines[2]\n",
      "    return headlines\n",
      "    \n",
      "def get_annotated_docs(headlines, annotated_file):\n",
      "    with open(annotated_file, 'r') as annotated:\n",
      "        annotated_lines = annotated.readlines()\n",
      "        annotated_docs = defaultdict(list)\n",
      "        annotated_doc_count = 0\n",
      "        for annotated_line in annotated_lines:\n",
      "            original_line = headlines[annotated_doc_count+1].strip() # headlines start counting at 1\n",
      "            # annotation not only adds '/xyz' to a word, but also messes up spacing\n",
      "            cleaned_annotated_line = ' '.join(ANNOTATION_REGEX.sub('', annotated_line).strip().split())\n",
      "        \n",
      "            if original_line == cleaned_annotated_line:\n",
      "                annotated_doc_count += 1\n",
      "            annotated_docs[annotated_doc_count].append(annotated_line)\n",
      "        return annotated_docs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unannotated_docs = get_unannotated_docs(UNANNOTATED_FILE)\n",
      "headlines = get_headlines(unannotated_docs)\n",
      "annotated_das_docs = get_annotated_docs(headlines, DAS_FILE)\n",
      "annotated_es_docs = get_annotated_docs(headlines, ES_FILE)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(docs) == len(annotated_das_docs) == len(annotated_es_docs)\n",
      "print len(docs), len(annotated_das_docs), len(annotated_das_docs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n",
        "170 170 170\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Match annotated documents to tokenized text files (and their MAZ IDs!)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fixed: Two files (00001, 00002) did have their headlines on the first line (instead of the third).\n",
      "\n",
      "## we need more than title matching\n",
      "\n",
      "There are two documents with the same headline:\n",
      "\n",
      "```\n",
      "maz-18160.tok.txt\n",
      "3:Alarmsignal\n",
      "5:Es brodelt an der CDU-Basis in Werder und Umgebung ...\n",
      "\n",
      "maz-18914.tok.txt\n",
      "3:Alarmsignal\n",
      "5:Mit Helga Kaden streicht eine der namhaftesten Gesch\u00e4ftsleute der Stadt die Segel ...\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_tokenized_docs(tokenized_files):\n",
      "    tokenized_docs = defaultdict(lambda : defaultdict()) # defaultdict of defaultdicts\n",
      "    for fpath in tokenized_files:\n",
      "        doc_id = re.search('\\d+', os.path.basename(fpath)).group()\n",
      "        tokenized_docs[doc_id]['path'] = fpath\n",
      "        with open(fpath, 'r') as tokenized_file:\n",
      "            tokenized_docs[doc_id]['lines'] = tokenized_file.readlines()\n",
      "            tokenized_docs[doc_id]['headline'] = tokenized_docs[doc_id]['lines'][2]\n",
      "    return tokenized_docs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unannotated_headlines = get_headlines(unannotated_docs)\n",
      "tokenized_docs = get_tokenized_docs(TOKENIZED_FILES)\n",
      "\n",
      "def map_unannotated_to_tokenized(unannotated_headlines, tokenized_docs):\n",
      "    headline_to_unanno_doc_id = defaultdict(list)\n",
      "    for unannotated_doc_id, unannotated_headline in unannotated_headlines.items():\n",
      "        headline_to_unanno_doc_id[unannotated_headline].append(unannotated_doc_id)\n",
      "    \n",
      "    unannotated_to_tokenized = defaultdict(list)\n",
      "    for i, tokenized_doc_id in enumerate(sorted(tokenized_docs.keys())):\n",
      "        tokenized_headline = tokenized_docs[tokenized_doc_id]['headline']\n",
      "        if tokenized_headline in headline_to_unanno_doc_id:\n",
      "            unannotated_doc_ids = headline_to_unanno_doc_id[tokenized_headline]\n",
      "            for unannotated_doc_id in unannotated_doc_ids:\n",
      "                unannotated_to_tokenized[unannotated_doc_id].append(tokenized_doc_id)\n",
      "        else:\n",
      "            for unanno_headline, unannotated_doc_ids in headline_to_unanno_doc_id.items():\n",
      "                if difflib.SequenceMatcher(None, tokenized_headline, unanno_headline).ratio() >= 0.9:\n",
      "                    for unannotated_doc_id in unannotated_doc_ids:\n",
      "                        unannotated_to_tokenized[unannotated_doc_id].append(tokenized_doc_id)\n",
      "                    break\n",
      "    return unannotated_to_tokenized, headline_to_unanno_doc_id"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unannotated_to_tokenized, headline_to_unanno_doc_id = map_unannotated_to_tokenized(unannotated_headlines, tokenized_docs)\n",
      "\n",
      "unannotated_docs = get_unannotated_docs(UNANNOTATED_FILE)\n",
      "for unannotated_doc_id in unannotated_docs:\n",
      "    if unannotated_doc_id not in unannotated_to_tokenized:\n",
      "        print \"unannotated_doc_id: \", unannotated_doc_id, unannotated_docs[unannotated_doc_id][2]\n",
      "        print \"\\tdoesn't (closely) match any tokenized headline\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(unannotated_docs)\n",
      "print len(unannotated_to_tokenized.keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "170\n",
        "170\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The headline 'Alarmsignale' is used in two texts:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(headline_to_unanno_doc_id)\n",
      "for headline in headline_to_unanno_doc_id:\n",
      "    if len(headline_to_unanno_doc_id[headline]) != 1:\n",
      "        print headline.strip(), headline_to_unanno_doc_id[headline]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "169\n",
        "Alarmsignal [84, 95]\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def disambiguate_matches(unannotated_to_tokenized, unannotated_docs, tokenized_docs):\n",
      "    \"\"\"\n",
      "    returns a dictionary that unambigiously maps from document IDs\n",
      "    used by Christian Dittrichs anaphora annotation tool to the document IDs\n",
      "    officially used in MAZ176.\n",
      "    \"\"\"\n",
      "    for unannotated_doc_id, tokenized_doc_ids in unannotated_to_tokenized.items():\n",
      "        # there's more than one tokenized text with the same headline\n",
      "        if len(tokenized_doc_ids) > 1:\n",
      "            unannotated_text = unannotated_docs[unannotated_doc_id][4]\n",
      "            candidate_scores = []\n",
      "            for tokenized_doc_id in tokenized_doc_ids:\n",
      "                tokenized_text = tokenized_docs[tokenized_doc_id]['lines'][4]\n",
      "                candidate_score = difflib.SequenceMatcher(None, unannotated_text, tokenized_text).ratio()\n",
      "                candidate_scores.append((tokenized_doc_id, candidate_score))\n",
      "            # choose the most likely candidate (based on string similarity of the text)\n",
      "            # max over values, cf. http://stackoverflow.com/a/13145419\n",
      "            unannotated_to_tokenized[unannotated_doc_id] = max(candidate_scores,key=itemgetter(1))[0]\n",
      "        else: # there's only one doc with this headline\n",
      "            unannotated_to_tokenized[unannotated_doc_id] = tokenized_doc_ids[0]\n",
      "    \n",
      "    return unannotated_to_tokenized"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Matching seems to work fine now, regardless of the tokenization:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokenized_docs = get_tokenized_docs(TOKENIZED_FILES)\n",
      "unannotated_to_tokenized, _ = map_unannotated_to_tokenized(unannotated_headlines, tokenized_docs)\n",
      "\n",
      "unannotated_docs = get_unannotated_docs(UNANNOTATED_FILE)\n",
      "\n",
      "unanno2tok = disambiguate_matches(unannotated_to_tokenized, unannotated_docs, tokenized_docs)\n",
      "\n",
      "for unanno_doc_id, tok_doc_id in unanno2tok.items():\n",
      "    unanno_headline = unannotated_docs[unanno_doc_id][2]\n",
      "    tok_headline = tokenized_docs[tok_doc_id]['lines'][2]\n",
      "    if unanno_headline != tok_headline:\n",
      "        print unanno_headline, tok_headline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "F\u00fcrchtet euch nicht!\n",
        "F\u00fcrchtet euch nicht ! \n",
        "\n",
        "Steuer frei!\n",
        "Steuer frei ! \n",
        "\n",
        "\"Schmierentheater\"\n",
        "\" Schmierentheater \"\n",
        "\n",
        "Aber doch nicht bei uns!\n",
        "Aber doch nicht bei uns ! \n",
        "\n",
        "B\u00f6se, die nicht b\u00f6se sind\n",
        "B\u00f6se , die nicht b\u00f6se sind\n",
        "\n",
        "Eigentlich fehlt ja keiner...\n",
        "Eigentlich fehlt ja keiner ... \n",
        "\n",
        "Sp\u00e4hen, nicht spitzeln\n",
        "Sp\u00e4hen , nicht spitzeln\n",
        "\n",
        "Des einen Freud . . .\n",
        "Des einen Freud ... \n",
        "\n",
        "Nicht vergessen!\n",
        "Nicht vergessen ! \n",
        "\n",
        "Gebt endlich Gummi!\n",
        "Gebt endlich Gummi ! \n",
        "\n",
        "Vielleicht geht's gut\n",
        "Vielleicht geht 's gut\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unannotated_docs = get_unannotated_docs(UNANNOTATED_FILE)\n",
      "unannotated_headlines = get_headlines(unannotated_docs)\n",
      "tokenized_docs = get_tokenized_docs(TOKENIZED_FILES)\n",
      "\n",
      "unannotated_to_tokenized, _ = map_unannotated_to_tokenized(unannotated_headlines, tokenized_docs)\n",
      "unanno2tok = disambiguate_matches(unannotated_to_tokenized, unannotated_docs, tokenized_docs)\n",
      "\n",
      "annotated_das_docs = get_annotated_docs(unannotated_headlines, DAS_FILE)\n",
      "\n",
      "# write DAS annotation files\n",
      "# for das_doc_id in annotated_das_docs:\n",
      "#     tok_doc_id = unanno2tok[das_doc_id]\n",
      "#     output_filepath = os.path.join(OUTPUT_ROOTDIR, 'das',\n",
      "#                                    \"maz-{}.anaphora.das.txt\".format(tok_doc_id))\n",
      "#     with open(output_filepath, 'w') as outfile:\n",
      "#         outfile.writelines(annotated_das_docs[das_doc_id])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# write ESS annotation files\n",
      "# annotated_es_docs = get_annotated_docs(headlines, ES_FILE)\n",
      "# for es_doc_id in annotated_es_docs:\n",
      "#     tok_doc_id = unanno2tok[es_doc_id]\n",
      "#     output_filepath = os.path.join(OUTPUT_ROOTDIR, 'es',\n",
      "#                                    \"maz-{}.anaphora.es.txt\".format(tok_doc_id))\n",
      "#     with open(output_filepath, 'w') as outfile:\n",
      "#         outfile.writelines(annotated_es_docs[es_doc_id])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Repair spacing in annotation files\n",
      "\n",
      "no space after annotation:\n",
      "```\n",
      "maz-14654.anaphora.das.txt\n",
      "Elisabeth Herzog-von der Heide (SPD) freute sich \u00fcber das/nWahlergebnis, das/rihr mehr als 60 Prozent einbrachte\n",
      "```\n",
      "\n",
      "too much space after annotation:\n",
      "```\n",
      "maz-7690.anaphora.das.txt\n",
      "Das/n  Schlachtefest in Paaren/Glien st\u00fctzt diese These.\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "annotations = {'n': 'nominal',\n",
      "               'a': 'abstract',\n",
      "               'r': 'relative',\n",
      "               'p': 'pleonastic'}\n",
      "\n",
      "no_space = \"das/nWahlergebnis, das/r?ihr mehr als das/a?fetzt\"\n",
      "too_much = \"Das/n  Schlachtefest in Paaren/Glien, das/a?  fetzt!\"\n",
      "\n",
      "NO_SPACE_AFTER_DAS_REGEX = re.compile('([Dd]as/[anr]\\??)(\\w)')\n",
      "NO_SPACE_AFTER_ES_REGEX = re.compile('([Ee]s/[anp]\\??)(\\w)')\n",
      "NO_SPACE_AFTER_ANNOTATION_REPL = r'\\1 \\2'\n",
      "\n",
      "print NO_SPACE_AFTER_DAS_REGEX.sub(NO_SPACE_AFTER_ANNOTATION_REPL, no_space)\n",
      "print NO_SPACE_AFTER_DAS_REGEX.sub(NO_SPACE_AFTER_ANNOTATION_REPL, too_much)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "das/n Wahlergebnis, das/r? ihr mehr als das/a? fetzt\n",
        "Das/n  Schlachtefest in Paaren/Glien, das/a?  fetzt!\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Repair DAS annotated files:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DAS_DIR = os.path.join(OUTPUT_ROOTDIR, 'das')\n",
      "DAS_FILES = !ls $DAS_DIR/*.das.txt\n",
      "\n",
      "# for das_filepath in DAS_FILES:\n",
      "#     with open(das_filepath, 'r') as das_file:\n",
      "#         fixed_lines = []\n",
      "#         for line in das_file.readlines():\n",
      "#             tokenized_line = ' '.join(line.split())+'\\n' # remove superfluous spacing between tokens\n",
      "#             annotated_line = NO_SPACE_AFTER_DAS_REGEX.sub(NO_SPACE_AFTER_ANNOTATION_REPL, tokenized_line)\n",
      "#             fixed_lines.append(annotated_line)\n",
      "#     with open(das_filepath, 'w') as das_file:\n",
      "#         das_file.writelines(fixed_lines)\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Repair ES annotated files:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ES_DIR = os.path.join(OUTPUT_ROOTDIR, 'es')\n",
      "ES_FILES = !ls $ES_DIR/*.es.txt\n",
      "\n",
      "# for es_filepath in ES_FILES:\n",
      "#     with open(es_filepath, 'r') as es_file:\n",
      "#         fixed_lines = []\n",
      "#         for line in es_file.readlines():\n",
      "#             tokenized_line = ' '.join(line.split())+'\\n' # remove superfluous spacing between tokens\n",
      "#             annotated_line = NO_SPACE_AFTER_ES_REGEX.sub(NO_SPACE_AFTER_ANNOTATION_REPL, tokenized_line)\n",
      "#             fixed_lines.append(annotated_line)\n",
      "#     with open(es_filepath, 'w') as es_file:\n",
      "#         es_file.writelines(fixed_lines)\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Convert Annotation files into graphs\n",
      "\n",
      "## tokenize annotated files to match our base tokenization\n",
      "\n",
      "For whatever reason, Christian Dittrich's tool uses untokenized files as input!\n",
      "Stefanie Dipper's tokenizer seems to work well with these files,\n",
      "but we need to add an empty line between the headline and the rest of the document!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DAS_DIR = os.path.join(OUTPUT_ROOTDIR, 'das')\n",
      "DAS_FILES = !ls $DAS_DIR/*.das.txt\n",
      "\n",
      "ES_DIR = os.path.join(OUTPUT_ROOTDIR, 'es')\n",
      "ES_FILES = !ls $ES_DIR/*.es.txt\n",
      "\n",
      "# add an empty line between headline and text to each document\n",
      "# for filelist in (DAS_FILES, ES_FILES):\n",
      "#     for filepath in filelist:\n",
      "#         with open(filepath, 'r') as infile:\n",
      "#             lines = infile.readlines()\n",
      "#         with open(filepath, 'w') as outfile:\n",
      "#             outfile.write(lines[0]+'\\n')\n",
      "#             outfile.writelines(lines[1:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we're able to tokenize the annotated files with Stefanie Dipper's tokenizer:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "~/repos/pcc-annis-merged/maz176/anaphora/tosik/das $ ls *.txt | parallel ~/repos/dipper-tokenizer/tokenize.perl {} {.}.tok\n",
      "~/repos/pcc-annis-merged/maz176/anaphora/tosik/das $ ls *.tok | parallel mv {} {.}.txt\n",
      "\n",
      "~/repos/pcc-annis-merged/maz176/anaphora/tosik/es $ ls *.txt | parallel ~/repos/dipper-tokenizer/tokenize.perl {} {.}.tok\n",
      "~/repos/pcc-annis-merged/maz176/anaphora/tosik/es $ ls *.tok | parallel mv {} {.}.txt"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So far, we have used _get_annotated_docs(unannotated_headlines, DAS_FILE)_ \n",
      "and some regex voodoo to extract the annotated texts from one annotation file.\n",
      "\n",
      "Now, let's try to extract them from our generated output files:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.search('\\d+', 'maz-00001.txt').group()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "'00001'"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_annotated_docs_from_files(annotation_filepaths):\n",
      "    annotated_docs = {}\n",
      "    for anno_filepath in annotation_filepaths:\n",
      "        maz_id = re.search('\\d+', os.path.basename(anno_filepath)).group()\n",
      "        with open(anno_filepath, 'r') as anno_file:\n",
      "            annotated_docs[maz_id] = anno_file.readlines()\n",
      "    return annotated_docs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try to match the annotated files against the tokenized ones:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DAS_DIR = os.path.join(OUTPUT_ROOTDIR, 'das')\n",
      "das_files = !ls $DAS_DIR/*.das.txt\n",
      "\n",
      "tokenized_docs = get_tokenized_docs(TOKENIZED_FILES)\n",
      "annotated_das_docs = get_annotated_docs_from_files(das_files)\n",
      "\n",
      "ANNOTATED_DAS_REGEX = re.compile('([Dd]as)/[anr]\\??')\n",
      "ANNOTATED_DAS_REPL = r'\\1'\n",
      "\n",
      "for annotated_doc_id in annotated_das_docs:\n",
      "    annotated_lines = annotated_das_docs[annotated_doc_id]\n",
      "    tokenized_lines = tokenized_docs[annotated_doc_id]['lines']\n",
      "\n",
      "    annotated_tokens = list(itertools.chain.from_iterable(line.split() for line in annotated_lines))\n",
      "    cleaned_anno_tokens = [ANNOTATED_DAS_REGEX.sub(ANNOTATED_DAS_REPL, anno_token) for anno_token in annotated_tokens]  \n",
      "    original_tokens = list(itertools.chain.from_iterable(line.split() for line in tokenized_lines))\n",
      "    \n",
      "    for i, orig_token in enumerate(original_tokens):\n",
      "        if orig_token != cleaned_anno_tokens[i]:\n",
      "            print \" geany\", \\\n",
      "                os.path.join(TOKENIZED_DATA_DIR, \"maz-{}.tok.txt\".format(annotated_doc_id)), \\\n",
      "                os.path.join(DAS_DIR, \"maz-{}.anaphora.das.txt\".format(annotated_doc_id)), \\\n",
      "                os.path.join(ES_DIR, \"maz-{}.anaphora.es.txt\".format(annotated_doc_id))\n",
      "            print \"token {0} not equal: {1} (original) vs. {2} (annotated)\".format(i, orig_token, cleaned_anno_tokens[i])\n",
      "            try:\n",
      "                print \"context: {0} __ {1}\\n\\n\".format(original_tokens[i-1], original_tokens[i+1])\n",
      "            except:\n",
      "                pass\n",
      "            break\n",
      "print \"finished alignment.\"\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "finished alignment.\n"
       ]
      }
     ],
     "prompt_number": 152
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After lots of manual fixes, this seems to work. Now, let's try to actually parse the files into graph representations..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ANNOTATED_ANAPHORA_REGEX = re.compile('(?P<token>([Dd]a|[Ee])s)/(?P<annotation>[anpr])(?P<certainty>\\??)')\n",
      "\n",
      "a = \"das/a\"\n",
      "b = \"es/n?\"\n",
      "c = \"Das/r?\"\n",
      "d = \"Es/p\"\n",
      "\n",
      "for token in (a,b,c,d):\n",
      "    regex_match = ANNOTATED_ANAPHORA_REGEX.search(token)\n",
      "    if regex_match:\n",
      "        print \"{0} --> {1}\".format(regex_match.group(), ANNOTATED_ANAPHORA_REGEX.sub(r'\\1', token))\n",
      "        print regex_match.group('token')\n",
      "        print regex_match.group('annotation')\n",
      "        if not regex_match.group('certainty'):\n",
      "            print \"certainty: 1.0\"\n",
      "        else:\n",
      "            print \"certainty: 0.5\"\n",
      "    else:\n",
      "        print token, \" didn't match.\"\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "das/a --> das\n",
        "das\n",
        "a\n",
        "certainty: 1.0\n",
        "es/n? --> es\n",
        "es\n",
        "n\n",
        "certainty: 0.5\n",
        "Das/r? --> Das\n",
        "Das\n",
        "r\n",
        "certainty: 0.5\n",
        "Es/p --> Es\n",
        "Es\n",
        "p\n",
        "certainty: 1.0\n"
       ]
      }
     ],
     "prompt_number": 184
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import itertools\n",
      "\n",
      "# Das/das/Es/es followed by an annotation (i.e. /a /n /p /r)\n",
      "# and optionally a '?' if the annotator was uncertain\n",
      "ANNOTATED_ANAPHORA_REGEX = re.compile('(?P<token>([Dd]a|[Ee])s)/(?P<annotation>[anpr])(?P<uncertain>\\??)')\n",
      "\n",
      "ANNOTATION_TYPES = {'n': 'nominal',\n",
      "                    'a': 'abstract',\n",
      "                    'r': 'relative',\n",
      "                    'p': 'pleonastic'}\n",
      "\n",
      "class AnaphoraDocumentGraph(MultiDiGraph):\n",
      "    \"\"\"\n",
      "    represents a text annotated with abstract anaphora types\n",
      "    as a graph\n",
      "    \"\"\"\n",
      "    def __init__(self, anaphora_filepath):\n",
      "        # super calls __init__() of base class MultiDiGraph\n",
      "        super(AnaphoraDocumentGraph, self).__init__()\n",
      "        self.name = os.path.basename(anaphora_filepath)\n",
      "        root_node_name = 'anaphoricity:root_node'\n",
      "        self.add_node(root_node_name, layers={'anaphoricity'})\n",
      "        \n",
      "        with open(anaphora_filepath, 'r') as anno_file:\n",
      "            annotated_lines = anno_file.readlines()\n",
      "            annotated_tokens = list(itertools.chain.from_iterable(line.split()\n",
      "                                                                      for line in annotated_lines))\n",
      "            for i, token in enumerate(annotated_tokens):\n",
      "                regex_match = ANNOTATED_ANAPHORA_REGEX.search(token)\n",
      "                if regex_match:\n",
      "                    unannotated_token = regex_match.group('token')\n",
      "                    annotation = regex_match.group('annotation')\n",
      "                    certainty = 1.0 if not regex_match.group('uncertain') else 0.5\n",
      "                    self.add_node(i, token=unannotated_token,\n",
      "                                  annotation={'anaphoricity': ANNOTATION_TYPES[annotation],\n",
      "                                              'certainty':certainty},\n",
      "                                  layers={'anaphoricity', 'anaphoricity:token'})\n",
      "                else: # token is not annotated\n",
      "                    self.add_node(i, token=token, layers={'anaphoricity', 'anaphoricity:token'})\n",
      "                self.add_edge(root_node_name, i, layers={'anaphoricity', 'anaphoricity:token'})\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 200
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for das_filepath in DAS_FILES[:2]:\n",
      "    adg = AnaphoraDocumentGraph(das_filepath)\n",
      "    print networkx.info(adg)\n",
      "    for node_id, node_data in adg.nodes_iter(data=True):\n",
      "        if 'anaphoricity' in node_data:\n",
      "            print node_data['token'], node_data['anaphoricity'], node_data['certainty']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Name: maz-10110.anaphora.das.txt\n",
        "Type: AnaphoraDocumentGraph\n",
        "Number of nodes: 191\n",
        "Number of edges: 190\n",
        "Average in degree:   0.9948\n",
        "Average out degree:   0.9948\n",
        "Das abstract 1.0\n",
        "das nominal 1.0\n",
        "Das abstract 1.0\n",
        "Das abstract 1.0\n",
        "Name: maz-10175.anaphora.das.txt\n",
        "Type: AnaphoraDocumentGraph\n",
        "Number of nodes: 208\n",
        "Number of edges: 207\n",
        "Average in degree:   0.9952\n",
        "Average out degree:   0.9952\n",
        "Das nominal 1.0\n",
        "das relative 1.0\n",
        "Das nominal 1.0\n",
        "das abstract 1.0\n"
       ]
      }
     ],
     "prompt_number": 204
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# We're done here. Switch to anaphoricity.py!"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}